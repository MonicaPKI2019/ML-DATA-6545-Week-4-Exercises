{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"},"colab":{"provenance":[{"file_id":"1-uI7R-yM1KxhrnS609Cs_qYooKDGWaBr","timestamp":1676596093416},{"file_id":"1-lN5RVLFbSAFfTPsd0Bq6QyEnrfyJO2k","timestamp":1645128942994},{"file_id":"12DYkAhatriK3nkILjOkkaRmo7T5GrK8K","timestamp":1613509984387}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-U6qNDSXBhPH"},"source":["# Feature Selection and Engineering - Beginner's Guide\n","\n","You all have seen datasets. Sometimes they are small, but often at times, they are tremendously large in size. It becomes very challenging to process the datasets which are very large, at least significant enough to cause a processing bottleneck.\n","\n","So, what makes these datasets this large? Well, it's features. The more the number of features the larger the datasets will be. Well, not always. You will find datasets where the number of features is very much, but they do not contain that many instances. But that is not the point of discussion here. So, you might wonder with a commodity computer in hand how to process these type of datasets without beating the bush.\n","\n","Often, in a high dimensional dataset, there remain some entirely irrelevant, insignificant and unimportant features. It has been seen that the contribution of these types of features is often less towards predictive modeling as compared to the critical features. They may have zero contribution as well. These features cause a number of problems which in turn prevents the process of efficient predictive modeling -\n","\n","- Unnecessary resource allocation for these features.\n","- These features act as a noise for which the machine learning model can perform terribly poorly.\n","- The machine model takes more time to get trained.\n","\n","So, what's the solution here? The most economical solution is Feature Selection.\n","\n","Feature Selection is the process of selecting out the most significant features from a given dataset. In many of the cases, Feature Selection can enhance the performance of a machine learning model as well.\n","\n","Sounds interesting right?\n","\n","You got an informal introduction to Feature Selection and its importance in the world of Data Science and Machine Learning. In this post you are going to cover:\n","\n","- Introduction to feature selection and understanding its importance\n","- Difference between feature selection and dimensionality reduction\n","- Different types of feature selection methods\n","- Implementation of different feature selection methods with scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"NLtKIzfDBhPg"},"source":["## A Case study in Python\n","For this case study, you will use the Pima Indians Diabetes dataset. The description of the dataset can be found [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database). \n","\n","The dataset corresponds to classification tasks on which you need to predict if a person has diabetes based on 8 features.\n","\n","There are a total of `768` observations in the dataset. Your first task is to load the dataset so that you can proceed. But before that let's import the necessary dependencies, you are going to need. You can import the other ones as you go along."]},{"cell_type":"code","metadata":{"id":"AjswS0FUBhPi","executionInfo":{"status":"ok","timestamp":1676669676352,"user_tz":300,"elapsed":1050,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kh4IZpAEBhP6"},"source":["Now that the dependencies are imported let's load the *Pima Indians* dataset into a `Dataframe` object with the help of `Pandas` library."]},{"cell_type":"code","metadata":{"id":"uyUX_YRBBhQE","executionInfo":{"status":"ok","timestamp":1676669676356,"user_tz":300,"elapsed":12,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"source":["#data = pd.read_csv(\"./data/diabetes.csv\")\n","data = pd.read_csv(\"https://raw.githubusercontent.com/DrJieTao/ba545-docs/master/data/diabetes.csv\")"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWoKbOP6BhQw"},"source":["The dataset is successfully loaded into the Dataframe object data. Now, let's take a look at the data."]},{"cell_type":"code","metadata":{"id":"7Z4DmV3RBhQz","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1676669681108,"user_tz":300,"elapsed":191,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"bbe3432d-6fe3-46be-eb9c-57c16854406c"},"source":["data.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0            6      148             72             35        0  33.6   \n","1            1       85             66             29        0  26.6   \n","2            8      183             64              0        0  23.3   \n","3            1       89             66             23       94  28.1   \n","4            0      137             40             35      168  43.1   \n","\n","   DiabetesPedigreeFunction  Age  Outcome  \n","0                     0.627   50        1  \n","1                     0.351   31        0  \n","2                     0.672   32        1  \n","3                     0.167   21        0  \n","4                     2.288   33        1  "],"text/html":["\n","  <div id=\"df-3782d7fd-a2a7-421e-a394-1e29eebe36ab\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3782d7fd-a2a7-421e-a394-1e29eebe36ab')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3782d7fd-a2a7-421e-a394-1e29eebe36ab button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3782d7fd-a2a7-421e-a394-1e29eebe36ab');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Wp4sJR03BhRl"},"source":["So you can see 8 different features labeled into the outcomes of `1` and `0` where `1` stands for the observation has diabetes, and `0` denotes the observation does not have diabetes. The dataset is known to have missing values. Specifically, there are missing observations for some columns that are marked as a zero value. You can deduce this by the definition of those columns, and it is impractical to have a zero value is invalid for those measures, e.g., zero for body mass index or blood pressure is invalid.\n","\n","But for this tutorial, you will directly use the preprocessed version of the dataset."]},{"cell_type":"code","metadata":{"id":"acSe0im_BhR4","executionInfo":{"status":"ok","timestamp":1676669689480,"user_tz":300,"elapsed":222,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"source":["# load data\n","url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = pd.read_csv(url, names=names)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ioY9S24BhSa"},"source":["You loaded the data in a DataFrame object called dataframe now.\n","\n","Let's convert the DataFrame object to a NumPy array to achieve faster computation. Also, let's segregate the data into separate variables so that the features and the labels are separated."]},{"cell_type":"code","metadata":{"id":"-mBNJBaxBhSc","executionInfo":{"status":"ok","timestamp":1676669692510,"user_tz":300,"elapsed":175,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"source":["array = dataframe.values\n","X = array[:,0:8] #features\n","y = array[:,8] #target"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mqr0iGyBhUS"},"source":["Wonderful! You have prepared your data.\n","\n","First, you will implement a *Chi-Squared* statistical test for non-negative features to select **4** of the best features from the dataset. You have already seen Chi-Squared test belongs the class of filter methods. If anyone's curious about knowing the internals of Chi-Squared, this [video](https://www.youtube.com/watch?v=VskmMgXmkMQ) does an excellent job.\n","\n","The `scikit-learn` library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features, in this case, it is Chi-Squared.\n","\n"]},{"cell_type":"code","metadata":{"id":"6jipra4tBhUT","executionInfo":{"status":"ok","timestamp":1676669763288,"user_tz":300,"elapsed":1570,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"source":["# Import the necessary libraries first\n","#### Correlation method - for classification chi2 only\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","#### RFE method - replace the model with the model you plan to use\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","#### feature importance method\n","#### this method can be used for both topK and cut-off\n","from sklearn.linear_model import Ridge\n","#### specifically tree-based feature importance method\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","#### for voting\n","from collections import Counter\n","#### for ANOVA\n","import statsmodels.formula.api as smf\n","import statsmodels.stats.api as sms"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Pearson's R Correlation based\n","- Used between continuous features"],"metadata":{"id":"E9PUtto2pEjZ"}},{"cell_type":"code","source":["def correlation_feat(df, threshold=0.5):\n","    \"\"\"Returns features that are correlated.\n","    (Set of all the names of correlated columns)\n","    \n","    Param:\n","    ------\n","    df: Pandas DataFrame\n","    threshold: float. Default, 0.75\n","        threshold for the correlation\n","    \"\"\"\n","    feature_corr = {}\n","    corr_matrix = df.corr()\n","    for i in range(len(corr_matrix.columns)):\n","        for j in range(i):\n","            # absolute coeff value\n","            if abs(corr_matrix.iloc[i, j]\n","                   ) > threshold:  \n","                # getting the name of column\n","                colname = corr_matrix.columns[i]  \n","                feature_corr[colname] = round(corr_matrix.iloc[i, j],3)\n","    return feature_corr"],"metadata":{"id":"S_rvfT6IpOSu","executionInfo":{"status":"ok","timestamp":1676669783988,"user_tz":300,"elapsed":156,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["x_df = dataframe.iloc[:,0:8]\n","correlation_feat(x_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q72cmn0lpQmS","executionInfo":{"status":"ok","timestamp":1676669802142,"user_tz":300,"elapsed":147,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"0704c5b5-8137-4a8a-94c5-c030cffd71c5"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'age': 0.544}"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### ANOVA\n","\n","- Used between continuous and categorical features\n","- use `f_classif` as the `score_func` in the `SelectKBest()` function"],"metadata":{"id":"7QdWXc4_p3OS"}},{"cell_type":"markdown","source":["```python\n","params = {\"corr\": 0.1, \"rfe\": 0: ,...}\n","```"],"metadata":{"id":"f_4VhLY4tW6s"}},{"cell_type":"code","source":["def feature_selector(X, y, model, names, _method=\"topk\", n=int(X.shape[1]/2), fit_X=False, thres = 0.1):\n","  \"\"\"voting based feature selector\n","  - _method: \"topk\" for top-K method (default), \"cutoff\" for cut-off based method\n","  - n: number of features to be selected. only available for top-K method - default half of the total features\n","  - thres: cut-off threshold (default 0.1), only availabel for cutoff method\n","  - fit_X: fit_transform X or just return indices\n","  TODO: include support for regression problems\n","  \"\"\"\n","  #### Placeholder for tests\n","  # X.shape[0] == y.shape[0]\n","  if names: # if given feature names\n","    feature_names = np.array(names)\n","  else: # otherwise use location\n","    feature_names = np.array([\"X%s\" % x for x in range(len(X.shape[0]))])\n","  if _method == 'topk':\n","    #### np.argpartition gets the indices of the largest n element from the array in ascending order\n","    #### [::-1] reverse the order\n","    ############################################################################\n","    #### IMPORTANT: \n","    #### use `chi2` as the `score_func` when doing categorical features - values has to be non-negative\n","    #### use `f_classif` as the `score_func` when doing categorical and continuous feautures\n","    #### use `f_regression` as the `score_func` when doing continuous feautures\n","    ############################################################################\n","    corr_features = list(np.argpartition(SelectKBest(score_func=chi2, k=n).fit(X,y).scores_, -n)[-n:][::-1])\n","    #### binary masking on features, use np.where() to get the indices of selected\n","    rfe_features = list(np.where(RFE(model, n_features_to_select=n, step=1).fit(X, y).support_)[0])\n","    #### argsort get sorted indices by values, ::-1] reverse the asceding order\n","    ridge_features = list(np.argsort(Ridge(alpha=1.0).fit(X, y).coef_)[-n:][::-1])\n","    #### using ExtraTree\n","    extratree_features = list(np.argsort(ExtraTreesClassifier().fit(X, y).feature_importances_)[-n:][::-1])\n","    ### using Random Forest:\n","    rf_features = list(np.argsort(RandomForestClassifier().fit(X, y).feature_importances_)[-n:][::-1])\n","    print(\"Using the Top-K method: \")\n","    print(\"Selected features by correlation: \", feature_names[corr_features])\n","    print(\"Selected features by RFE: \", feature_names[rfe_features])\n","    print(\"Selected features by Ridge coefficients: \", feature_names[ridge_features])\n","    print(\"Selected features by Extra Tree feature importance: \", feature_names[extratree_features])\n","    print(\"Selected features by Random Forest feature importance: \", feature_names[rf_features])\n","\n","  elif _method == 'cutoff':\n","    corr_features, rfe_features = list(), list() #### N/A\n","    ridge_features = list(np.where(Ridge(alpha=1.0).fit(X, y).coef_ > thres)[0])\n","    extratree_features = list(np.where(ExtraTreesClassifier().fit(X, y).feature_importances_ > thres)[0])\n","    rf_features = list(np.where(RandomForestClassifier().fit(X, y).feature_importances_ > thres)[0])\n","    print(\"Using the Cutoff method: \")\n","    # print(\"Selected features by correlation: \", feature_names[corr_features])\n","    # print(\"Selected features by RFE: \", feature_names[rfe_features])\n","    print(\"Selected features by Ridge coefficients: \", feature_names[ridge_features])\n","    print(\"Selected features by Extra Tree feature importance: \", feature_names[extratree_features])\n","    print(\"Selected features by Random Forest feature importance: \", feature_names[rf_features])\n","  else:\n","    return(\"Only Top-K and Cutoff methods are currently supported!\")\n","\n","  ######################################\n","  #### combine results using voting ####\n","  ######################################\n","  counted = Counter(np.concatenate((corr_features, rfe_features, ridge_features, extratree_features, rf_features), axis=None)).most_common(n)\n","\n","  #### list of tuples (feature_index, votes)\n","  counted.sort(key = lambda x: x[1], reverse=True)\n","  final_select_series = pd.Series({feature_names[f]:c for f,c in counted}).sort_values(ascending=False)\n","  selected_idx = sorted([f for f,c in counted])\n","  assert len(selected_idx) == n\n","  print(final_select_series)\n","  if fit_X:\n","    return(X[:,selected_idx])\n","  else:\n","    return(feature_names[selected_idx])\n","\n","\n","\n"],"metadata":{"id":"fav6s04GV_lZ","executionInfo":{"status":"ok","timestamp":1676669821795,"user_tz":300,"elapsed":167,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["selected_features = feature_selector(X, y, model=RandomForestClassifier(), names=names, fit_X=True)\n","selected_features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pglq-w-Ogt_6","executionInfo":{"status":"ok","timestamp":1676669834332,"user_tz":300,"elapsed":2087,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"20fdca24-8d65-4e7b-bbb6-c5b1a31f38e0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Using the Top-K method: \n","Selected features by correlation:  ['test' 'plas' 'age' 'mass']\n","Selected features by RFE:  ['plas' 'mass' 'pedi' 'age']\n","Selected features by Ridge coefficients:  ['pedi' 'preg' 'mass' 'plas']\n","Selected features by Extra Tree feature importance:  ['plas' 'age' 'mass' 'pedi']\n","Selected features by Random Forest feature importance:  ['plas' 'mass' 'age' 'pedi']\n","plas    5\n","mass    5\n","age     4\n","pedi    4\n","dtype: int64\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[148.   ,  33.6  ,   0.627,  50.   ],\n","       [ 85.   ,  26.6  ,   0.351,  31.   ],\n","       [183.   ,  23.3  ,   0.672,  32.   ],\n","       ...,\n","       [121.   ,  26.2  ,   0.245,  30.   ],\n","       [126.   ,  30.1  ,   0.349,  47.   ],\n","       [ 93.   ,  30.4  ,   0.315,  23.   ]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### Feature selection Pipelines\n","Use this if you want to compare different feature selection strategies (e.g., ANOVA vs. correlation). You can also use this method to get a rough idea of how many features should be selected."],"metadata":{"id":"arG6qGVmtr_F"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","from sklearn.pipeline import Pipeline\n","from matplotlib import pyplot"],"metadata":{"id":"oYK4rMk5uKu2","executionInfo":{"status":"ok","timestamp":1676669864522,"user_tz":300,"elapsed":148,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# evaluate a give model using cross-validation\n","def evaluate_model(model, X, y):\n"," cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n"," scores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n"," return scores"],"metadata":{"id":"iOr0ICPFuEW3","executionInfo":{"status":"ok","timestamp":1676669872659,"user_tz":300,"elapsed":155,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# define number of features to evaluate\n","num_features = [i+1 for i in range(X.shape[1])]\n","# enumerate each number of features\n","results = list()\n","feat_dict = {}\n","for k in num_features:\n"," # create pipeline\n"," model = LogisticRegression(solver='liblinear')\n"," fs = SelectKBest(score_func=f_classif, k=k)\n"," pipeline = Pipeline(steps=[('anova',fs), ('lr', model)])\n"," # evaluate the model\n"," scores = evaluate_model(pipeline, X, y)\n"," results.append(scores)\n"," feat_dict[k] = (1-np.mean(scores),np.std(scores))\n"," # summarize the results\n"," print('selecting {:1d} features with a bias of {:.3f} and a variance of {:.3f}'.format(k, 1-np.mean(scores),np.std(scores)))\n","# plot model performance for comparison\n","pyplot.boxplot(results, labels=num_features, showmeans=True);\n","# pyplot.show()\n","biases = np.array([v[0] for v in feat_dict.values()])\n","best_bias = np.argsort(biases)[0]\n","print(\"best bias of {:.3f} with {:1d} features\".format(biases[best_bias],best_bias+1))\n","viariances = np.array([v[1] for v in feat_dict.values()])\n","best_var = np.argsort(viariances)[0]\n","print(\"best variance of {:.3f} with {:1d} features\".format(viariances[best_var],best_var+1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"LC4si7NruRdA","executionInfo":{"status":"ok","timestamp":1676669879290,"user_tz":300,"elapsed":2950,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"143387b3-f8c0-4165-9b03-f65891255428"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["selecting 1 features with a bias of 0.435 and a variance of 0.089\n","selecting 2 features with a bias of 0.421 and a variance of 0.079\n","selecting 3 features with a bias of 0.398 and a variance of 0.081\n","selecting 4 features with a bias of 0.400 and a variance of 0.077\n","selecting 5 features with a bias of 0.383 and a variance of 0.071\n","selecting 6 features with a bias of 0.393 and a variance of 0.076\n","selecting 7 features with a bias of 0.382 and a variance of 0.076\n","selecting 8 features with a bias of 0.380 and a variance of 0.074\n","best bias of 0.380 with 8 features\n","best variance of 0.071 with 5 features\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["In this case, it looks like selecting five and seven features results in roughly the same accuracy. So we can select **5** features.\n","\n","__NOTE__: We used `f1` as the metric here. You can change it to different metrics to see how the result changes"],"metadata":{"id":"pYNRhnXYuo_m"}},{"cell_type":"code","source":["### if you want the top 3 values for K\n","k = 3\n","best_bias = np.argsort(biases)[:k]+1\n","best_bias\n","# biases[best_bias]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5clgb75Cy9mc","executionInfo":{"status":"ok","timestamp":1676669950143,"user_tz":300,"elapsed":150,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"3a294673-c459-4c5e-eb56-950946a91083"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([8, 7, 5])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["The result shows that the best bias is with all 8 features, then 7 features, then 5. \n","\n","You can do something similar for variance."],"metadata":{"id":"n2k-jgRU0vDf"}},{"cell_type":"markdown","source":["## NEED MORE HELP?\n","Refer to [this article](https://towardsdatascience.com/beginners-guide-for-feature-selection-by-a-beginner-cd2158c5c36a) and [this article](https://machinelearningmastery.com/feature-selection-with-numerical-input-data/) if you need more help."],"metadata":{"id":"Lek0xp5htJc1"}},{"cell_type":"code","source":[],"metadata":{"id":"oYwf_MvWnlXv"},"execution_count":null,"outputs":[]}]}